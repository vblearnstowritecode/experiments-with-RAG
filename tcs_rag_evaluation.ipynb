{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TCS RAG System Evaluation with LLM Scoring\n\nThis notebook evaluates our TCS Annual Report RAG system by using GPT-4.1 as an independent evaluator.\nThe LLM will score both Original and Query Expansion approaches on a 0/1 scale with rationales.\n\n## Evaluation Process:\n1. Connect to existing ChromaDB vector database\n2. Load existing evaluation results CSV\n3. For each question, retrieve 8 most relevant chunks from TCS report\n4. Show GPT-4.1 the question + both answers + 8 relevant chunks\n5. Get binary scores (0/1) and explanations for each answer\n6. Update CSV with LLM evaluation results\n7. Analyze patterns and compare approaches"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Setup & Dependencies\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport chromadb\nfrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\nimport time\nfrom datetime import datetime\n\n# Load environment variables\nload_dotenv()\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# File paths\nCHROMA_DB_PATH = \"./chroma_db\"\nCSV_PATH = \"tcs_rag_evaluation_20250927_214934.csv\"\nCOLLECTION_NAME = \"tcs_annual_report_2024\"\n\nprint(\"‚úÖ Dependencies loaded successfully!\")\nprint(f\"üóÉÔ∏è  ChromaDB Path: {CHROMA_DB_PATH}\")\nprint(f\"üìä CSV Path: {CSV_PATH}\")\nprint(f\"üìö Collection: {COLLECTION_NAME}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Connect to ChromaDB Vector Database\ndef connect_to_chromadb(chroma_path, collection_name):\n    \"\"\"\n    Connect to existing ChromaDB collection from the main RAG notebook.\n    \"\"\"\n    print(f\"üóÉÔ∏è  Connecting to ChromaDB at {chroma_path}...\")\n\n    # Initialize ChromaDB client\n    chroma_client = chromadb.PersistentClient(path=chroma_path)\n\n    # Create embedding function (same as main notebook)\n    embedding_function = SentenceTransformerEmbeddingFunction()\n\n    try:\n        # Get existing collection\n        chroma_collection = chroma_client.get_collection(\n            collection_name,\n            embedding_function=embedding_function\n        )\n\n        # Verify collection\n        count = chroma_collection.count()\n        print(f\"‚úÖ Connected to existing collection: {collection_name}\")\n        print(f\"üìä Total document chunks: {count}\")\n\n        # Test a sample query to ensure everything works\n        test_results = chroma_collection.query(\n            query_texts=[\"TCS revenue\"],\n            n_results=2\n        )\n        print(f\"üß™ Test query successful - retrieved {len(test_results['documents'][0])} chunks\")\n\n        return chroma_collection\n\n    except Exception as e:\n        print(f\"‚ùå Failed to connect to ChromaDB collection: {str(e)}\")\n        print(f\"üí° Make sure you've run the main RAG notebook first to create the collection\")\n        raise\n\n# Connect to the vector database\nchroma_collection = connect_to_chromadb(CHROMA_DB_PATH, COLLECTION_NAME)\n\nprint(f\"\\nüìã Vector Database Ready:\")\nprint(f\"   Collection: {COLLECTION_NAME}\")\nprint(f\"   Chunks available: {chroma_collection.count():,}\")\nprint(f\"   Ready for 8-chunk evaluation! üöÄ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load CSV and Prepare for Evaluation\n",
    "def load_and_prepare_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Load the evaluation CSV and add columns for LLM scores.\n",
    "    \"\"\"\n",
    "    print(f\"üìä Loading evaluation data from {csv_path}...\")\n",
    "    \n",
    "    # Load existing CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Add new columns for LLM evaluation if they don't exist\n",
    "    new_columns = [\n",
    "        'llm_original_score',\n",
    "        'llm_expansion_score', \n",
    "        'llm_original_rationale',\n",
    "        'llm_expansion_rationale'\n",
    "    ]\n",
    "    \n",
    "    for col in new_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "            print(f\"   ‚ûï Added column: {col}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Column exists: {col}\")\n",
    "    \n",
    "    print(f\"\\nüìã Dataset Summary:\")\n",
    "    print(f\"   Total questions: {len(df)}\")\n",
    "    print(f\"   Difficulty breakdown:\")\n",
    "    difficulty_counts = df['difficulty'].value_counts()\n",
    "    for difficulty, count in difficulty_counts.items():\n",
    "        print(f\"     {difficulty}: {count} questions\")\n",
    "    \n",
    "    # Check how many questions already have LLM scores\n",
    "    evaluated_count = df['llm_original_score'].notna().sum()\n",
    "    remaining_count = len(df) - evaluated_count\n",
    "    \n",
    "    print(f\"\\nüîç Evaluation Status:\")\n",
    "    print(f\"   Already evaluated: {evaluated_count}\")\n",
    "    print(f\"   Remaining: {remaining_count}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and prepare the dataframe\n",
    "df = load_and_prepare_csv(CSV_PATH)\n",
    "\n",
    "# Display first few questions for reference\n",
    "print(f\"\\nüìù Sample Questions:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    row = df.iloc[i]\n",
    "    print(f\"   {i+1}. [{row['difficulty']}] {row['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: LLM Evaluation Function with 8-Chunk Retrieval\ndef evaluate_single_question(question, original_answer, expansion_answer, chroma_collection):\n    \"\"\"\n    Evaluate both answers for a single question using GPT-4.1.\n    Retrieves 8 most relevant chunks from ChromaDB for context.\n\n    Args:\n        question: The question being asked\n        original_answer: Answer from original RAG approach\n        expansion_answer: Answer from query expansion approach\n        chroma_collection: ChromaDB collection with TCS report chunks\n\n    Returns:\n        dict: Contains scores, rationales, and retrieved chunks\n    \"\"\"\n\n    # Retrieve 8 most relevant chunks for this question\n    try:\n        retrieval_results = chroma_collection.query(\n            query_texts=[question],\n            n_results=8\n        )\n\n        if not retrieval_results['documents'][0]:\n            print(f\"‚ö†Ô∏è  No relevant chunks found for question: {question}\")\n            context_chunks = [\"No relevant context found in TCS report.\"]\n        else:\n            context_chunks = retrieval_results['documents'][0]\n\n        # Combine chunks into context\n        context = \"\\n\\n--- Chunk ---\\n\\n\".join(context_chunks)\n\n    except Exception as e:\n        print(f\"‚ùå Error retrieving chunks: {str(e)}\")\n        context = \"Error retrieving context from TCS report.\"\n        context_chunks = []\n\n    evaluation_prompt = f\"\"\"You are an expert evaluator with access to relevant excerpts from the TCS Annual Report.\n\nQuestion: {question}\n\nAnswer A (Original RAG): {original_answer}\n\nAnswer B (Query Expansion RAG): {expansion_answer}\n\nRelevant TCS Annual Report Context (8 most relevant chunks):\n{context}\n\nFor each answer, provide:\n1. Score (1 = factually correct and well-supported by the provided context, 0 = incorrect/inaccurate/unsupported)\n2. Brief rationale explaining the score based on the context above\n\nFormat your response exactly as follows:\nOriginal Answer Score: [0 or 1]\nOriginal Rationale: [explanation]\nExpansion Answer Score: [0 or 1]\nExpansion Rationale: [explanation]\"\"\"\n\n    try:\n        response = client.responses.create(\n            model=\"gpt-4.1\",\n            input=evaluation_prompt\n        )\n\n        content = response.output_text if hasattr(response, 'output_text') else str(response)\n\n        # Parse the response\n        lines = content.strip().split('\\n')\n\n        results = {\n            'original_score': None,\n            'original_rationale': None,\n            'expansion_score': None,\n            'expansion_rationale': None,\n            'retrieved_chunks': len(context_chunks),\n            'context_preview': context[:200] + \"...\" if len(context) > 200 else context\n        }\n\n        for line in lines:\n            if line.startswith('Original Answer Score:'):\n                results['original_score'] = int(line.split(':')[1].strip())\n            elif line.startswith('Original Rationale:'):\n                results['original_rationale'] = line.split(':', 1)[1].strip()\n            elif line.startswith('Expansion Answer Score:'):\n                results['expansion_score'] = int(line.split(':')[1].strip())\n            elif line.startswith('Expansion Rationale:'):\n                results['expansion_rationale'] = line.split(':', 1)[1].strip()\n\n        return results\n\n    except Exception as e:\n        print(f\"‚ùå Error during evaluation: {str(e)}\")\n        return {\n            'original_score': None,\n            'original_rationale': f\"Error: {str(e)}\",\n            'expansion_score': None,\n            'expansion_rationale': f\"Error: {str(e)}\",\n            'retrieved_chunks': 0,\n            'context_preview': \"Error occurred\"\n        }\n\nprint(\"‚úÖ LLM evaluation function ready!\")\nprint(\"üîß Function will:\")\nprint(\"   ‚Ä¢ Retrieve 8 most relevant chunks for each question\")\nprint(\"   ‚Ä¢ Send question + both answers + 8 chunks to GPT-4.1\")\nprint(\"   ‚Ä¢ Get 0/1 scores and rationales for each answer\")\nprint(\"   ‚Ä¢ Show which chunks were used for evaluation\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Question-by-Question Evaluation\ndef process_single_question(df, question_idx, chroma_collection, save_progress=True):\n    \"\"\"\n    Process a single question and update the dataframe.\n    \n    Args:\n        df: DataFrame containing questions and answers\n        question_idx: Index of question to process\n        chroma_collection: ChromaDB collection for chunk retrieval\n        save_progress: Whether to save CSV after evaluation\n    \n    Returns:\n        bool: True if evaluation was successful\n    \"\"\"\n    row = df.iloc[question_idx]\n    \n    # Check if already evaluated\n    if pd.notna(row['llm_original_score']):\n        print(f\"‚è≠Ô∏è  Question {question_idx + 1} already evaluated, skipping...\")\n        return True\n    \n    print(f\"\\nüîç Evaluating Question {question_idx + 1}/{len(df)}\")\n    print(f\"üìã Difficulty: {row['difficulty']}\")\n    print(f\"‚ùì Question: {row['question']}\")\n    print(\"-\" * 60)\n    \n    # Get evaluation results\n    start_time = time.time()\n    results = evaluate_single_question(\n        row['question'],\n        row['original_answer'],\n        row['expansion_answer'],\n        chroma_collection\n    )\n    evaluation_time = time.time() - start_time\n    \n    # Update dataframe\n    if results['original_score'] is not None:\n        df.loc[question_idx, 'llm_original_score'] = results['original_score']\n        df.loc[question_idx, 'llm_expansion_score'] = results['expansion_score']\n        df.loc[question_idx, 'llm_original_rationale'] = results['original_rationale']\n        df.loc[question_idx, 'llm_expansion_rationale'] = results['expansion_rationale']\n        \n        # Display results\n        print(f\"\\nüìä Evaluation Results (took {evaluation_time:.1f}s):\")\n        print(f\"   üìö Retrieved {results.get('retrieved_chunks', 'Unknown')} chunks for context\")\n        print(f\"   üîµ Original Answer Score: {results['original_score']}\")\n        print(f\"   üîµ Original Rationale: {results['original_rationale'][:100]}{'...' if len(results['original_rationale']) > 100 else ''}\")\n        print(f\"   üü† Expansion Answer Score: {results['expansion_score']}\")\n        print(f\"   üü† Expansion Rationale: {results['expansion_rationale'][:100]}{'...' if len(results['expansion_rationale']) > 100 else ''}\")\n        \n        # Save progress\n        if save_progress:\n            df.to_csv(CSV_PATH, index=False)\n            print(f\"üíæ Progress saved to {CSV_PATH}\")\n        \n        return True\n    else:\n        print(f\"‚ùå Evaluation failed for question {question_idx + 1}\")\n        return False\n\nprint(\"‚úÖ Question processing function ready!\")\nprint(\"üîß Function will:\")\nprint(\"   ‚Ä¢ Skip already-evaluated questions\")\nprint(\"   ‚Ä¢ Retrieve 8 relevant chunks per question\")\nprint(\"   ‚Ä¢ Show progress and results for each question\")\nprint(\"   ‚Ä¢ Save progress after each successful evaluation\")\nprint(\"   ‚Ä¢ Handle errors gracefully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process Questions One by One\n# You can run this cell multiple times - it will skip already-evaluated questions\n\nprint(\"üöÄ Starting LLM Evaluation Process\")\nprint(\"=\" * 50)\nprint(\"üí° Run this cell to process the next unevaluated question\")\nprint(\"üîÑ Re-run to continue with subsequent questions\")\nprint(\"‚è≠Ô∏è  Already-evaluated questions will be skipped automatically\")\nprint()\n\n# Find next unevaluated question\nunevaluated_mask = df['llm_original_score'].isna()\nunevaluated_indices = df[unevaluated_mask].index.tolist()\n\nif unevaluated_indices:\n    next_idx = unevaluated_indices[0]\n    print(f\"üìã Processing next question: {next_idx + 1}/{len(df)}\")\n    print(f\"üìä Remaining questions: {len(unevaluated_indices)}\")\n    \n    # Process the question\n    success = process_single_question(df, next_idx, chroma_collection)\n    \n    if success:\n        remaining = len(unevaluated_indices) - 1\n        print(f\"\\n‚úÖ Question {next_idx + 1} completed successfully!\")\n        print(f\"üìà Progress: {len(df) - remaining}/{len(df)} questions evaluated\")\n        \n        if remaining > 0:\n            print(f\"üîÑ Re-run this cell to evaluate the next question ({remaining} remaining)\")\n        else:\n            print(f\"üéâ All questions have been evaluated!\")\n    else:\n        print(f\"‚ùå Failed to evaluate question {next_idx + 1}\")\n        print(f\"üîÑ You can try re-running this cell to retry\")\n        \nelse:\n    print(\"üéâ All questions have already been evaluated!\")\n    print(\"üìä Ready for analysis and summary\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Analysis and Summary\n",
    "def analyze_evaluation_results(df):\n",
    "    \"\"\"\n",
    "    Analyze the LLM evaluation results and provide insights.\n",
    "    \"\"\"\n",
    "    print(\"üìä LLM EVALUATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Filter for evaluated questions only\n",
    "    evaluated_df = df[df['llm_original_score'].notna()].copy()\n",
    "    \n",
    "    if len(evaluated_df) == 0:\n",
    "        print(\"‚ùå No questions have been evaluated yet.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìã Total Evaluated Questions: {len(evaluated_df)}/{len(df)}\")\n",
    "    print()\n",
    "    \n",
    "    # Overall accuracy\n",
    "    original_accuracy = evaluated_df['llm_original_score'].mean()\n",
    "    expansion_accuracy = evaluated_df['llm_expansion_score'].mean()\n",
    "    \n",
    "    print(\"üéØ Overall Accuracy:\")\n",
    "    print(f\"   üîµ Original RAG: {original_accuracy:.1%} ({evaluated_df['llm_original_score'].sum()}/{len(evaluated_df)})\")\n",
    "    print(f\"   üü† Query Expansion: {expansion_accuracy:.1%} ({evaluated_df['llm_expansion_score'].sum()}/{len(evaluated_df)})\")\n",
    "    \n",
    "    improvement = expansion_accuracy - original_accuracy\n",
    "    if improvement > 0:\n",
    "        print(f\"   ‚úÖ Query Expansion is {improvement:.1%} better\")\n",
    "    elif improvement < 0:\n",
    "        print(f\"   üìâ Query Expansion is {abs(improvement):.1%} worse\")\n",
    "    else:\n",
    "        print(f\"   üü∞ Both approaches perform equally\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Accuracy by difficulty\n",
    "    print(\"üìà Accuracy by Difficulty Level:\")\n",
    "    for difficulty in ['Easy', 'Medium', 'Hard']:\n",
    "        subset = evaluated_df[evaluated_df['difficulty'] == difficulty]\n",
    "        if len(subset) > 0:\n",
    "            orig_acc = subset['llm_original_score'].mean()\n",
    "            exp_acc = subset['llm_expansion_score'].mean()\n",
    "            print(f\"   {difficulty:6}: Original {orig_acc:.1%} | Expansion {exp_acc:.1%} ({len(subset)} questions)\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Questions where approaches differ\n",
    "    different_scores = evaluated_df[evaluated_df['llm_original_score'] != evaluated_df['llm_expansion_score']]\n",
    "    print(f\"üîÑ Questions with Different Scores: {len(different_scores)}/{len(evaluated_df)}\")\n",
    "    \n",
    "    if len(different_scores) > 0:\n",
    "        print(\"\\nüìã Questions where approaches differed:\")\n",
    "        for idx, row in different_scores.iterrows():\n",
    "            orig_score = int(row['llm_original_score'])\n",
    "            exp_score = int(row['llm_expansion_score'])\n",
    "            winner = \"Expansion\" if exp_score > orig_score else \"Original\"\n",
    "            print(f\"   Q{row['question_id']}: Original={orig_score}, Expansion={exp_score} ‚Üí {winner} wins\")\n",
    "            print(f\"      {row['question'][:80]}{'...' if len(row['question']) > 80 else ''}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Updated results saved to: {CSV_PATH}\")\n",
    "    print(f\"üìä CSV now contains LLM scores and rationales for analysis\")\n",
    "    \n",
    "    return evaluated_df\n",
    "\n",
    "# Run the analysis\n",
    "analysis_df = analyze_evaluation_results(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
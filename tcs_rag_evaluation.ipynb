{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# TCS RAG System Evaluation Questions\n",
    "\n",
    "This notebook contains our evaluation question set for testing the TCS Annual Report RAG system.\n",
    "We have 10 carefully designed questions across three difficulty levels to test different aspects of our RAG pipeline.\n",
    "\n",
    "## Question Distribution:\n",
    "- **Easy (3 questions)**: Direct fact extraction from the report\n",
    "- **Medium (4 questions)**: Analysis and synthesis requiring multiple chunks\n",
    "- **Hard (3 questions)**: Complex reasoning and multi-hop information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & Dependencies for ChromaDB connection\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "# ChromaDB configuration\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"tcs_annual_report_2024\"\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded successfully!\")\n",
    "print(f\"üóÉÔ∏è  ChromaDB Path: {CHROMA_DB_PATH}\")\n",
    "print(f\"üìö Collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ChromaDB Vector Database\n",
    "def connect_to_chromadb(chroma_path, collection_name):\n",
    "    \"\"\"\n",
    "    Connect to existing ChromaDB collection from the main RAG notebook.\n",
    "    \"\"\"\n",
    "    print(f\"üóÉÔ∏è  Connecting to ChromaDB at {chroma_path}...\")\n",
    "\n",
    "    # Initialize ChromaDB client\n",
    "    chroma_client = chromadb.PersistentClient(path=chroma_path)\n",
    "\n",
    "    # Create embedding function (same as main notebook)\n",
    "    embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "\n",
    "    try:\n",
    "        # Get existing collection\n",
    "        chroma_collection = chroma_client.get_collection(\n",
    "            collection_name,\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "\n",
    "        # Verify collection\n",
    "        count = chroma_collection.count()\n",
    "        print(f\"‚úÖ Connected to existing collection: {collection_name}\")\n",
    "        print(f\"üìä Total document chunks: {count:,}\")\n",
    "\n",
    "        return chroma_collection\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to ChromaDB collection: {str(e)}\")\n",
    "        print(f\"üí° Make sure you've run the main RAG notebook first to create the collection\")\n",
    "        raise\n",
    "\n",
    "# Connect to the vector database\n",
    "chroma_collection = connect_to_chromadb(CHROMA_DB_PATH, COLLECTION_NAME)\n",
    "\n",
    "print(f\"\\nüöÄ ChromaDB Ready for Evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCS RAG Evaluation Questions\n",
    "evaluation_questions = [\n",
    "    # Easy Questions (3) - Direct fact extraction\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"difficulty\": \"Easy\",\n",
    "        \"question\": \"What was TCS's total revenue in FY 2025?\",\n",
    "        \"expected_answer_hint\": \"US $30 billion or ‚Çπ255,324 crore\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2, \n",
    "        \"difficulty\": \"Easy\",\n",
    "        \"question\": \"How many associates does TCS have globally?\",\n",
    "        \"expected_answer_hint\": \"607,979 associates\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"difficulty\": \"Easy\", \n",
    "        \"question\": \"What was the total dividend per share for FY 2025?\",\n",
    "        \"expected_answer_hint\": \"‚Çπ126 per share\"\n",
    "    },\n",
    "    \n",
    "    # Medium Questions (4) - Analysis and synthesis\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"difficulty\": \"Medium\",\n",
    "        \"question\": \"What are TCS's main business segments or industry verticals?\",\n",
    "        \"expected_answer_hint\": \"Banking Financial Services, Manufacturing, Communications Media, Life Sciences Healthcare Energy, etc.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"difficulty\": \"Medium\",\n",
    "        \"question\": \"What was TCS's operating margin in FY 2025 and how does it compare to the previous year?\",\n",
    "        \"expected_answer_hint\": \"24.3% in FY 2025, slight decline from FY 2024\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"difficulty\": \"Medium\", \n",
    "        \"question\": \"What are the key AI/GenAI initiatives mentioned in the report?\",\n",
    "        \"expected_answer_hint\": \"TCS WisdomNext platform, AI agents, drug discovery solutions, etc.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"difficulty\": \"Medium\",\n",
    "        \"question\": \"What major partnerships or client wins are highlighted in the report?\",\n",
    "        \"expected_answer_hint\": \"Air New Zealand, Xerox, Primark, DNB, etc.\"\n",
    "    },\n",
    "    \n",
    "    # Hard Questions (3) - Complex analysis and synthesis\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"difficulty\": \"Hard\",\n",
    "        \"question\": \"How is TCS positioning itself for the AI transformation, and what specific investments are they making?\",\n",
    "        \"expected_answer_hint\": \"Largest AI-trained workforce, TCS WisdomNext platform, AI data centers, human+AI model, etc.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"difficulty\": \"Hard\", \n",
    "        \"question\": \"What are the key challenges TCS faced in FY 2025 and how did they address them?\",\n",
    "        \"expected_answer_hint\": \"Geopolitical disruption, supply chain impacts - addressed through client partnerships, technology transformation\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"difficulty\": \"Hard\",\n",
    "        \"question\": \"Based on the Chairman's letter, what are the four key progressions TCS plans for the future?\",\n",
    "        \"expected_answer_hint\": \"1) AI agents pool, 2) Human+AI delivery, 3) AI data center investments, 4) Industry partnerships\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üìã TCS RAG EVALUATION QUESTIONS READY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total questions: {len(evaluation_questions)}\")\n",
    "print(f\"Easy: {len([q for q in evaluation_questions if q['difficulty'] == 'Easy'])}\")\n",
    "print(f\"Medium: {len([q for q in evaluation_questions if q['difficulty'] == 'Medium'])}\")\n",
    "print(f\"Hard: {len([q for q in evaluation_questions if q['difficulty'] == 'Hard'])}\")\n",
    "print()\n",
    "\n",
    "# Display all questions for review\n",
    "for q in evaluation_questions:\n",
    "    print(f\"Q{q['id']} [{q['difficulty']}]: {q['question']}\")\n",
    "    print(f\"    Expected: {q['expected_answer_hint']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval for a sample question to verify everything works\n",
    "sample_question = evaluation_questions[0]['question']\n",
    "print(f\"üß™ Testing retrieval with sample question:\")\n",
    "print(f\"Question: {sample_question}\")\n",
    "print()\n",
    "\n",
    "# Query ChromaDB for relevant chunks\n",
    "results = chroma_collection.query(query_texts=[sample_question], n_results=3)\n",
    "\n",
    "if results['documents'][0]:\n",
    "    print(f\"‚úÖ Successfully retrieved {len(results['documents'][0])} relevant chunks:\")\n",
    "    for i, doc in enumerate(results['documents'][0], 1):\n",
    "        print(f\"\\nChunk {i}:\")\n",
    "        print(doc[:150] + \"...\" if len(doc) > 150 else doc)\n",
    "else:\n",
    "    print(\"‚ùå No relevant chunks found\")\n",
    "\n",
    "print(\"\\nüéØ Ready to test RAG system with these evaluation questions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Setup Environment and Import RAG Functions\nimport os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport time\nimport pandas as pd\n\n# Load environment variables\nload_dotenv()\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nprint(\"‚úÖ Environment setup complete!\")\nprint(\"üîå OpenAI client initialized\")\n\n# Import the clean RAG functions from our new module\ntry:\n    from tcs_rag import basic_rag, query_expansion_rag, multiple_queries_rag\n    from sentence_transformers import CrossEncoder\n    \n    # Initialize cross-encoder for multiple_queries_rag\n    print(\"ü§ñ Loading cross-encoder model...\")\n    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    \n    print(\"‚úÖ Successfully imported RAG functions from tcs_rag.py!\")\n    print(\"üìã Functions available:\")\n    print(\"   ‚Ä¢ basic_rag(question, chroma_collection, client)\")\n    print(\"   ‚Ä¢ query_expansion_rag(question, chroma_collection, client)\")\n    print(\"   ‚Ä¢ multiple_queries_rag(question, chroma_collection, client, cross_encoder)\")\n    \nexcept ImportError as e:\n    print(f\"‚ùå Failed to import RAG functions: {e}\")\n    print(\"üí° Make sure tcs_rag.py is in the same directory\")\n    raise\n\nprint(\"\\nüéØ Ready for RAG evaluation!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Remove old complex import cell - now using clean tcs_rag.py import\nprint(\"üóëÔ∏è  Old import method removed - now using clean tcs_rag.py imports!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Single Question Evaluation Test\nprint(\"üß™ SINGLE QUESTION EVALUATION TEST\")\nprint(\"=\" * 50)\n\n# Test with Question 1 (Easy)\ntest_question = evaluation_questions[0]\nquestion = test_question['question']\nexpected = test_question['expected_answer_hint']\ndifficulty = test_question['difficulty']\n\nprint(f\"Question ID: {test_question['id']}\")\nprint(f\"Difficulty: {difficulty}\")\nprint(f\"Question: {question}\")\nprint(f\"Expected: {expected}\")\nprint()\n\n# Define methods with updated function calls (passing required parameters)\nmethods = [\n    (\"Basic RAG\", lambda q: basic_rag(q, chroma_collection, client)),\n    (\"Query Expansion\", lambda q: query_expansion_rag(q, chroma_collection, client)),\n    (\"Multiple Queries\", lambda q: multiple_queries_rag(q, chroma_collection, client, cross_encoder))\n]\n\nresults = []\n\nfor method_name, method_func in methods:\n    print(f\"üîÑ Running {method_name}...\")\n    \n    # Get answer with timing\n    result = method_func(question)\n    \n    results.append({\n        'Method': method_name,\n        'Answer': result['answer'],\n        'Time_seconds': result['runtime']\n    })\n    \n    print(f\"‚úÖ {method_name} completed in {result['runtime']}s\")\n    print()\n\nprint(\"üìã All three methods completed! Now running judge evaluation...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge LLM Evaluation\n",
    "def judge_answer(question, expected_answer, actual_answer):\n",
    "    \"\"\"\n",
    "    Use GPT-4.1 as judge to evaluate an answer\n",
    "    \"\"\"\n",
    "    judge_prompt = f\"\"\"You are evaluating a RAG system answer about the TCS Annual Report.\n",
    "\n",
    "Question: {question}\n",
    "Expected Answer: {expected_answer}\n",
    "Actual Answer: {actual_answer}\n",
    "\n",
    "Evaluate this answer:\n",
    "- Score 1 if factually correct and aligns with expected answer\n",
    "- Score 0 if incorrect, inaccurate, or doesn't answer the question\n",
    "\n",
    "Format your response exactly as:\n",
    "Score: [0 or 1]\n",
    "Rationale: [brief explanation]\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            input=judge_prompt\n",
    "        )\n",
    "        \n",
    "        content = response.output_text if hasattr(response, 'output_text') else str(response)\n",
    "        \n",
    "        # Parse score and rationale\n",
    "        lines = content.strip().split('\\n')\n",
    "        score = None\n",
    "        rationale = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith('Score:'):\n",
    "                score = int(line.split(':')[1].strip())\n",
    "            elif line.startswith('Rationale:'):\n",
    "                rationale = line.split(':', 1)[1].strip()\n",
    "        \n",
    "        return score, rationale\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None, f\"Error: {str(e)}\"\n",
    "\n",
    "# Evaluate each answer\n",
    "print(\"ü§ñ Judge LLM Evaluation\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    method_name = result['Method']\n",
    "    answer = result['Answer']\n",
    "    \n",
    "    print(f\"\\nEvaluating {method_name}...\")\n",
    "    \n",
    "    score, rationale = judge_answer(question, expected, answer)\n",
    "    \n",
    "    # Add to results\n",
    "    results[i]['Score'] = score\n",
    "    results[i]['Rationale'] = rationale\n",
    "    \n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Rationale: {rationale}\")\n",
    "\n",
    "print(\"\\n‚úÖ Judge evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Results in DataFrame\n",
    "print(\"üìä EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display the full results\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print()\n",
    "\n",
    "# Show summary table\n",
    "summary_df = df_results[['Method', 'Score', 'Time_seconds']].copy()\n",
    "print(\"üìã Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Show detailed answers and rationales\n",
    "print(\"üìù Detailed Results:\")\n",
    "print()\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Method: {row['Method']}\")\n",
    "    print(f\"Score: {row['Score']} | Time: {row['Time_seconds']}s\")\n",
    "    print(f\"\\nAnswer:\")\n",
    "    print(row['Answer'])\n",
    "    print(f\"\\nJudge Rationale:\")\n",
    "    print(row['Rationale'])\n",
    "    print()\n",
    "\n",
    "# Simple analysis\n",
    "print(f\"\\nüèÜ RESULTS SUMMARY:\")\n",
    "print(f\"Correct answers: {df_results['Score'].sum()}/{len(df_results)}\")\n",
    "print(f\"Average time: {df_results['Time_seconds'].mean():.1f}s\")\n",
    "if df_results['Score'].sum() > 0:\n",
    "    best_methods = df_results[df_results['Score'] == 1]['Method'].tolist()\n",
    "    print(f\"Best performing methods: {', '.join(best_methods)}\")\n",
    "\n",
    "print(\"\\nüéØ Single question test complete! Ready to scale to all 10 questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "j3ctxzaibad",
   "source": "# Full Evaluation: All 10 Questions Across 3 RAG Methods\nprint(\"üöÄ FULL EVALUATION: ALL 10 QUESTIONS\")\nprint(\"=\" * 60)\n\n# Initialize results list for all evaluations\nall_results = []\n\n# Define methods with updated function calls\nmethods = [\n    (\"Basic RAG\", lambda q: basic_rag(q, chroma_collection, client)),\n    (\"Query Expansion\", lambda q: query_expansion_rag(q, chroma_collection, client)),\n    (\"Multiple Queries\", lambda q: multiple_queries_rag(q, chroma_collection, client, cross_encoder))\n]\n\n# Loop through all 10 questions\nfor q_num, test_question in enumerate(evaluation_questions, 1):\n    question = test_question['question']\n    expected = test_question['expected_answer_hint']\n    difficulty = test_question['difficulty']\n    question_id = test_question['id']\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"QUESTION {q_num}/10 [ID: {question_id}] - {difficulty.upper()}\")\n    print(f\"{'='*70}\")\n    print(f\"Q: {question}\")\n    print(f\"Expected: {expected}\")\n    print()\n    \n    # Test each RAG method for this question\n    for method_name, method_func in methods:\n        print(f\"üîÑ Running {method_name}...\")\n        \n        try:\n            # Get answer with timing\n            result = method_func(question)\n            answer = result['answer']\n            runtime = result['runtime']\n            \n            print(f\"‚úÖ {method_name} completed in {runtime}s\")\n            \n            # Judge evaluation\n            print(f\"ü§ñ Judging {method_name}...\")\n            score, rationale = judge_answer(question, expected, answer)\n            \n            # Store complete result\n            all_results.append({\n                'Question_ID': question_id,\n                'Question_Number': q_num,\n                'Difficulty': difficulty,\n                'Question': question,\n                'Expected_Answer': expected,\n                'Method': method_name,\n                'Answer': answer,\n                'Time_seconds': runtime,\n                'Score': score,\n                'Judge_Rationale': rationale\n            })\n            \n            print(f\"üìä Score: {score} | {rationale[:100]}...\")\n            \n        except Exception as e:\n            print(f\"‚ùå Error with {method_name}: {str(e)}\")\n            # Store error result\n            all_results.append({\n                'Question_ID': question_id,\n                'Question_Number': q_num,\n                'Difficulty': difficulty,\n                'Question': question,\n                'Expected_Answer': expected,\n                'Method': method_name,\n                'Answer': f\"ERROR: {str(e)}\",\n                'Time_seconds': 0,\n                'Score': 0,\n                'Judge_Rationale': f\"Error occurred: {str(e)}\"\n            })\n    \n    print(f\"‚úÖ Question {q_num} complete!\")\n\nprint(f\"\\nüéâ ALL EVALUATIONS COMPLETE!\")\nprint(f\"Total evaluations: {len(all_results)}\")\nprint(f\"Expected: {len(evaluation_questions) * len(methods)} (10 questions √ó 3 methods)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "yzoaxtsbf6j",
   "source": "# Create Comprehensive Results DataFrame and Export to CSV\nprint(\"üìä CREATING COMPREHENSIVE RESULTS DATAFRAME\")\nprint(\"=\" * 50)\n\n# Create DataFrame from all results\ndf_comprehensive = pd.DataFrame(all_results)\n\n# Display basic info\nprint(f\"üìã Total evaluations: {len(df_comprehensive)}\")\nprint(f\"üìã Questions evaluated: {df_comprehensive['Question_ID'].nunique()}\")\nprint(f\"üìã Methods tested: {df_comprehensive['Method'].nunique()}\")\nprint()\n\n# Show column info\nprint(\"üìã DataFrame columns:\")\nfor col in df_comprehensive.columns:\n    print(f\"   ‚Ä¢ {col}\")\nprint()\n\n# Quick summary stats\nprint(\"üìä QUICK SUMMARY:\")\nprint(f\"Overall success rate: {df_comprehensive['Score'].mean():.1%}\")\nprint(f\"Average response time: {df_comprehensive['Time_seconds'].mean():.1f}s\")\nprint()\n\n# Method performance summary\nmethod_summary = df_comprehensive.groupby('Method').agg({\n    'Score': ['mean', 'sum', 'count'],\n    'Time_seconds': 'mean'\n}).round(3)\n\nmethod_summary.columns = ['Success_Rate', 'Correct_Answers', 'Total_Questions', 'Avg_Time_seconds']\nprint(\"üìã Method Performance Summary:\")\nprint(method_summary)\nprint()\n\n# Difficulty analysis\ndifficulty_summary = df_comprehensive.groupby('Difficulty').agg({\n    'Score': ['mean', 'sum', 'count']\n}).round(3)\ndifficulty_summary.columns = ['Success_Rate', 'Correct_Answers', 'Total_Attempts']\nprint(\"üìã Performance by Difficulty:\")\nprint(difficulty_summary)\nprint()\n\n# Generate timestamp for unique filename\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\ncsv_filename = f\"tcs_rag_evaluation_{timestamp}.csv\"\n\n# Export to CSV\ndf_comprehensive.to_csv(csv_filename, index=False)\nprint(f\"üíæ Results exported to: {csv_filename}\")\n\n# Display first few rows to verify\nprint(\"\\\\nüìã Sample of exported data:\")\nprint(df_comprehensive[['Question_ID', 'Method', 'Score', 'Time_seconds']].head(10))\n\nprint(f\"\\\\n‚úÖ Comprehensive evaluation complete!\")\nprint(f\"üìÅ Full results saved to: {csv_filename}\")\nprint(f\"üìä {len(df_comprehensive)} total evaluations across {len(evaluation_questions)} questions and {len(methods)} methods\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "chrgwlobih5",
   "source": "# üèÜ FINAL SCORECARD: ALL METHODS ACROSS ALL 10 QUESTIONS\nprint(\"üèÜ FINAL SCORECARD: TCS RAG EVALUATION RESULTS\")\nprint(\"=\" * 60)\n\n# Overall Performance Summary\nprint(\"üìä OVERALL PERFORMANCE SUMMARY\")\nprint(\"-\" * 40)\n\nfinal_summary = df_comprehensive.groupby('Method').agg({\n    'Score': ['sum', 'count', 'mean'],\n    'Time_seconds': 'mean'\n}).round(3)\n\nfinal_summary.columns = ['Correct_Answers', 'Total_Questions', 'Accuracy_Rate', 'Avg_Time_seconds']\nfinal_summary['Accuracy_Percentage'] = (final_summary['Accuracy_Rate'] * 100).round(1)\n\nprint(final_summary[['Correct_Answers', 'Total_Questions', 'Accuracy_Percentage', 'Avg_Time_seconds']])\nprint()\n\n# Performance by Difficulty Level\nprint(\"üìà PERFORMANCE BY DIFFICULTY LEVEL\")\nprint(\"-\" * 40)\n\ndifficulty_breakdown = df_comprehensive.pivot_table(\n    index='Method', \n    columns='Difficulty', \n    values='Score', \n    aggfunc=['sum', 'count', 'mean']\n)\n\n# Show correct answers out of total for each difficulty\nfor difficulty in ['Easy', 'Medium', 'Hard']:\n    if difficulty in difficulty_breakdown['sum'].columns:\n        print(f\"\\n{difficulty} Questions:\")\n        for method in difficulty_breakdown.index:\n            correct = difficulty_breakdown['sum'][difficulty].loc[method]\n            total = difficulty_breakdown['count'][difficulty].loc[method]\n            percentage = (correct/total * 100) if total > 0 else 0\n            print(f\"  {method}: {correct}/{total} ({percentage:.1f}%)\")\n\nprint()\n\n# Winner Analysis\nprint(\"ü•á WINNER ANALYSIS\")\nprint(\"-\" * 40)\n\n# Overall winner\nbest_overall = final_summary.loc[final_summary['Accuracy_Rate'].idxmax()]\nprint(f\"üèÜ Overall Winner: {best_overall.name}\")\nprint(f\"   Score: {best_overall['Correct_Answers']:.0f}/{best_overall['Total_Questions']:.0f} ({best_overall['Accuracy_Percentage']:.1f}%)\")\nprint(f\"   Avg Time: {best_overall['Avg_Time_seconds']:.1f}s\")\n\n# Fastest method\nfastest_method = final_summary.loc[final_summary['Avg_Time_seconds'].idxmin()]\nprint(f\"\\n‚ö° Fastest Method: {fastest_method.name}\")\nprint(f\"   Avg Time: {fastest_method['Avg_Time_seconds']:.1f}s\")\nprint(f\"   Accuracy: {fastest_method['Accuracy_Percentage']:.1f}%\")\n\nprint()\n\n# Key Insights\nprint(\"üí° KEY INSIGHTS\")\nprint(\"-\" * 40)\n\ntotal_questions = len(evaluation_questions)\ntotal_methods = len(methods)\n\nprint(f\"‚Ä¢ Evaluated {total_methods} RAG methods across {total_questions} questions\")\nprint(f\"‚Ä¢ Overall success rate: {df_comprehensive['Score'].mean():.1%}\")\nprint(f\"‚Ä¢ Best performing method: {best_overall.name} ({best_overall['Accuracy_Percentage']:.1f}% accuracy)\")\nprint(f\"‚Ä¢ Speed vs Accuracy trade-off: {fastest_method.name} is fastest but {best_overall.name} is most accurate\")\n\n# Check if any method got perfect score\nperfect_methods = final_summary[final_summary['Accuracy_Rate'] == 1.0]\nif not perfect_methods.empty:\n    print(f\"‚Ä¢ Perfect score achieved by: {', '.join(perfect_methods.index)}\")\n\n# Difficulty insights\neasy_avg = df_comprehensive[df_comprehensive['Difficulty'] == 'Easy']['Score'].mean()\nmedium_avg = df_comprehensive[df_comprehensive['Difficulty'] == 'Medium']['Score'].mean()\nhard_avg = df_comprehensive[df_comprehensive['Difficulty'] == 'Hard']['Score'].mean()\n\nprint(f\"‚Ä¢ Difficulty progression: Easy ({easy_avg:.1%}) > Medium ({medium_avg:.1%}) > Hard ({hard_avg:.1%})\")\n\nprint(\"\\n‚úÖ EVALUATION COMPLETE!\")\nprint(f\"üìÅ Detailed results exported to: {csv_filename}\")\nprint(\"üéØ Use these insights to improve your RAG system!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
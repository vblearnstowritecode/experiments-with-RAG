{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCS Annual Report RAG System\n",
    "\n",
    "Learning embeddings through building a simple question-answering system for TCS Annual Report.\n",
    "\n",
    "This notebook follows a step-by-step approach to understand how embeddings work in retrieval-augmented generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Helper function for readable text display\n",
    "def word_wrap(text, width=80):\n",
    "    \"\"\"\n",
    "    Simple word wrap function to make long text readable.\n",
    "    Wraps text at word boundaries within the specified width.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if current_length + len(word) + len(current_line) > width:\n",
    "            if current_line:\n",
    "                lines.append(' '.join(current_line))\n",
    "                current_line = [word]\n",
    "                current_length = len(word)\n",
    "            else:\n",
    "                lines.append(word)\n",
    "                current_length = 0\n",
    "        else:\n",
    "            current_line.append(word)\n",
    "            current_length += len(word)\n",
    "    \n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# Test the function\n",
    "test_text = \"This is a very long sentence that we will use to test our word wrapping function to make sure it works correctly and makes text readable.\"\n",
    "print(word_wrap(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: PDF Reading - Extract text from TCS Annual Report\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Load the PDF and extract text from all pages\n",
    "reader = PdfReader(\"TCS_Annual_Report.pdf\")\n",
    "pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
    "\n",
    "# Filter out empty strings (blank pages)\n",
    "pdf_texts = [text for text in pdf_texts if text]\n",
    "\n",
    "print(f\"Total pages with content: {len(pdf_texts)}\")\n",
    "print(\"\\nFirst page content:\")\n",
    "print(\"=\" * 50)\n",
    "print(word_wrap(pdf_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Character Chunking - Split into 1000-character chunks with overlap\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create character splitter with 50-character overlap (improvement over reference)\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50  # Adding overlap to preserve context\n",
    ")\n",
    "\n",
    "# Join all pages and split into character chunks\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
    "\n",
    "print(\"Sample chunk (index 10):\")\n",
    "print(\"=\" * 40)\n",
    "print(word_wrap(character_split_texts[10]))\n",
    "print(f\"\\nTotal character chunks: {len(character_split_texts)}\")\n",
    "print(f\"First chunk length: {len(character_split_texts[0])} characters\")\n",
    "print(f\"Last chunk length: {len(character_split_texts[-1])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Token Chunking - Further split into 256-token chunks with overlap\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# Create token splitter with 20-token overlap (improvement over reference)\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=20,  # Adding overlap to preserve context\n",
    "    tokens_per_chunk=256\n",
    ")\n",
    "\n",
    "# Split each character chunk into token chunks\n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(\"Sample token chunk (index 10):\")\n",
    "print(\"=\" * 40)\n",
    "print(word_wrap(token_split_texts[10]))\n",
    "print(f\"\\nTotal token chunks: {len(token_split_texts)}\")\n",
    "\n",
    "# Let's also check a few more details\n",
    "print(f\"Character chunks: {len(character_split_texts)}\")\n",
    "print(f\"Token chunks: {len(token_split_texts)}\")\n",
    "print(f\"Ratio (token/char chunks): {len(token_split_texts)/len(character_split_texts):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Embedding Generation - Convert text chunks to numerical vectors\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "# Create embedding function (uses sentence-transformers model)\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "\n",
    "# Test with one chunk to see what embeddings look like\n",
    "sample_embedding = embedding_function([token_split_texts[10]])\n",
    "print(\"Sample embedding (first 10 values):\")\n",
    "print(sample_embedding[0][:10])\n",
    "print(f\"\\nEmbedding dimensions: {len(sample_embedding[0])}\")\n",
    "print(f\"Data type: {type(sample_embedding[0][0])}\")\n",
    "\n",
    "# Quick check - embeddings are normalized vectors (should sum to ~1.0 when squared)\n",
    "import numpy as np\n",
    "magnitude = np.linalg.norm(sample_embedding[0])\n",
    "print(f\"Vector magnitude (should be ~1.0): {magnitude:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Step 6: ChromaDB Setup - Create collection and store all document chunks\n# Use persistent storage in the repo directory\nchroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n\n# Create collection for TCS annual report (or get existing one)\ntry:\n    chroma_collection = chroma_client.get_collection(\n        \"tcs_annual_report_2024\",\n        embedding_function=embedding_function\n    )\n    print(\"üìÅ Using existing collection from disk\")\n    skip_adding = True\nexcept:\n    chroma_collection = chroma_client.create_collection(\n        \"tcs_annual_report_2024\",\n        embedding_function=embedding_function\n    )\n    print(\"üìÅ Created new persistent collection\")\n    skip_adding = False\n\n# Only add documents if we created a new collection\nif not skip_adding:\n    # Create IDs for each chunk (simple sequential numbering)\n    ids = [str(i) for i in range(len(token_split_texts))]\n    \n    # Add all chunks to the collection (this will generate embeddings for all chunks)\n    print(f\"Adding {len(token_split_texts)} chunks to ChromaDB...\")\n    chroma_collection.add(ids=ids, documents=token_split_texts)\n\n# Verify the collection\ncount = chroma_collection.count()\nprint(f\"‚úÖ Collection ready!\")\nprint(f\"Total documents in collection: {count}\")\nprint(f\"Collection name: {chroma_collection.name}\")\nprint(f\"Storage location: ./chroma_db/\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "25September2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
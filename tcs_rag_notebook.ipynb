{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# TCS Annual Report RAG System\n",
    "\n",
    "Learning embeddings through building a simple question-answering system for TCS Annual Report.\n",
    "\n",
    "This notebook follows a step-by-step approach to understand how embeddings work in retrieval-augmented generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Helper function for readable text display\n",
    "def word_wrap(text, width=80):\n",
    "    \"\"\"\n",
    "    Simple word wrap function to make long text readable.\n",
    "    Wraps text at word boundaries within the specified width.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if current_length + len(word) + len(current_line) > width:\n",
    "            if current_line:\n",
    "                lines.append(' '.join(current_line))\n",
    "                current_line = [word]\n",
    "                current_length = len(word)\n",
    "            else:\n",
    "                lines.append(word)\n",
    "                current_length = 0\n",
    "        else:\n",
    "            current_line.append(word)\n",
    "            current_length += len(word)\n",
    "    \n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# Test the function\n",
    "test_text = \"This is a very long sentence that we will use to test our word wrapping function to make sure it works correctly and makes text readable.\"\n",
    "print(word_wrap(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: PDF Reading - Extract text from TCS Annual Report\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Load the PDF and extract text from all pages\n",
    "reader = PdfReader(\"TCS_Annual_Report.pdf\")\n",
    "pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
    "\n",
    "# Filter out empty strings (blank pages)\n",
    "pdf_texts = [text for text in pdf_texts if text]\n",
    "\n",
    "print(f\"Total pages with content: {len(pdf_texts)}\")\n",
    "print(\"\\nFirst page content:\")\n",
    "print(\"=\" * 50)\n",
    "print(word_wrap(pdf_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Character Chunking - Split into 1000-character chunks with overlap\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create character splitter with 50-character overlap (improvement over reference)\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50  # Adding overlap to preserve context\n",
    ")\n",
    "\n",
    "# Join all pages and split into character chunks\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
    "\n",
    "print(\"Sample chunk (index 10):\")\n",
    "print(\"=\" * 40)\n",
    "print(word_wrap(character_split_texts[10]))\n",
    "print(f\"\\nTotal character chunks: {len(character_split_texts)}\")\n",
    "print(f\"First chunk length: {len(character_split_texts[0])} characters\")\n",
    "print(f\"Last chunk length: {len(character_split_texts[-1])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Token Chunking - Further split into 256-token chunks with overlap\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# Create token splitter with 20-token overlap (improvement over reference)\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=20,  # Adding overlap to preserve context\n",
    "    tokens_per_chunk=256\n",
    ")\n",
    "\n",
    "# Split each character chunk into token chunks\n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(\"Sample token chunk (index 10):\")\n",
    "print(\"=\" * 40)\n",
    "print(word_wrap(token_split_texts[10]))\n",
    "print(f\"\\nTotal token chunks: {len(token_split_texts)}\")\n",
    "\n",
    "# Let's also check a few more details\n",
    "print(f\"Character chunks: {len(character_split_texts)}\")\n",
    "print(f\"Token chunks: {len(token_split_texts)}\")\n",
    "print(f\"Ratio (token/char chunks): {len(token_split_texts)/len(character_split_texts):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Embedding Generation - Convert text chunks to numerical vectors\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "# Create embedding function (uses sentence-transformers model)\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "\n",
    "# Test with one chunk to see what embeddings look like\n",
    "sample_embedding = embedding_function([token_split_texts[10]])\n",
    "print(\"Sample embedding (first 10 values):\")\n",
    "print(sample_embedding[0][:10])\n",
    "print(f\"\\nEmbedding dimensions: {len(sample_embedding[0])}\")\n",
    "print(f\"Data type: {type(sample_embedding[0][0])}\")\n",
    "\n",
    "# Quick check - embeddings are normalized vectors (should sum to ~1.0 when squared)\n",
    "import numpy as np\n",
    "magnitude = np.linalg.norm(sample_embedding[0])\n",
    "print(f\"Vector magnitude (should be ~1.0): {magnitude:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Using existing collection from disk\n",
      "‚úÖ Collection ready!\n",
      "Total documents in collection: 1324\n",
      "Collection name: tcs_annual_report_2024\n",
      "Storage location: ./chroma_db/\n"
     ]
    }
   ],
   "source": [
    "# Step 6: ChromaDB Setup - Create collection and store all document chunks\n",
    "# Use persistent storage in the repo directory\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create collection for TCS annual report (or get existing one)\n",
    "try:\n",
    "    chroma_collection = chroma_client.get_collection(\n",
    "        \"tcs_annual_report_2024\",\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    print(\"üìÅ Using existing collection from disk\")\n",
    "    skip_adding = True\n",
    "except:\n",
    "    chroma_collection = chroma_client.create_collection(\n",
    "        \"tcs_annual_report_2024\",\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    print(\"üìÅ Created new persistent collection\")\n",
    "    skip_adding = False\n",
    "\n",
    "# Only add documents if we created a new collection\n",
    "if not skip_adding:\n",
    "    # Create IDs for each chunk (simple sequential numbering)\n",
    "    ids = [str(i) for i in range(len(token_split_texts))]\n",
    "    \n",
    "    # Add all chunks to the collection (this will generate embeddings for all chunks)\n",
    "    print(f\"Adding {len(token_split_texts)} chunks to ChromaDB...\")\n",
    "    chroma_collection.add(ids=ids, documents=token_split_texts)\n",
    "\n",
    "# Verify the collection\n",
    "count = chroma_collection.count()\n",
    "print(f\"‚úÖ Collection ready!\")\n",
    "print(f\"Total documents in collection: {count}\")\n",
    "print(f\"Collection name: {chroma_collection.name}\")\n",
    "print(f\"Storage location: ./chroma_db/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing semantic search retrieval:\n",
      "\n",
      "üîç Query: 'What is TCS's revenue?'\n",
      "============================================================\n",
      "üìÑ Found 3 relevant chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "( in fy 2021 ) and settlement ( in fy 2024 ) of legal claim tcs has consistently\n",
      "grown its earnings per share ( eps ), achieving a cagr of 9. 3 % over the past\n",
      "five financial years. this steady increase highlights the company ‚Äô s growing\n",
      "earnings and its commitment to delivering long - term value to shareholders.\n",
      "earnings per share\n",
      "\n",
      "--- Chunk 2 ---\n",
      "capabilities in independent analyst reports. tcs has been ranked as a leader in\n",
      "analyst competitive surveys across multiple firms covering areas such as ai,\n",
      "genai, analytics, data and automation. 1includes multiple investors in group\n",
      "meetings growth 12. 0 % $ 20. 2 $ 22. 7 fy 2021 fy 2025 fy 2021 fy 2025 228 298\n",
      "130 64 101 48 us $ 100mn + us $ 50mn + us $ 20mn + 148 182 fy 2024 fy 2025 tcs '\n",
      "global innovation network includes 11 pace ports and studios, fostering\n",
      "collaboration on cutting - edge solutions through cxo discussions, design\n",
      "thinking, and agile building. tcs engages with customers globally through\n",
      "summits, trade shows, and vendor partner events. integrated annual report 2024 -\n",
      "25 social capital 26\n",
      "\n",
      "--- Chunk 3 ---\n",
      "previous year ‚Äô s revenue from operations of ` 2, 02, 359 crore. the profit for\n",
      "the year attributable to shareholders in fy 2025 was ` 48, 057 crore registering\n",
      "a growth of 10. 3 % over the profit for the year attributable to shareholders of\n",
      "` 43, 559 crore in fy 2024. 5. subsidiary companies on march 31, 2025, the\n",
      "company has 52 subsidiaries and there has been no material change in the nature\n",
      "of the business of the subsidiaries. there are no associates or joint venture\n",
      "companies within the meaning of section 2 ( 6 ) of the companies act, 2013 ( ‚Äú\n",
      "the act ‚Äù ). tcs financial solutions ( beijing ) co., ltd. was merged with tata\n",
      "consultancy services ( china ) co., ltd. a step - down wholly owned subsidiary\n",
      "of the company w. e. f. july 1, 2024. on december 20, 2024, tata consultancy\n",
      "services ( africa ) ( proprietary ) limited ( ‚Äú tcs africa ‚Äù ), a wholly owned\n",
      "subsidiary of the company disposed off 30 % of its equity stake in tata\n",
      "consultancy services ( south africa ) ( proprietary )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Environment Setup & Retrieval Testing\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def test_retrieval(query, n_results=3):\n",
    "    \"\"\"\n",
    "    Test function to see what chunks our embeddings retrieve for a given query.\n",
    "    This helps us understand how semantic search works.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Query: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Query ChromaDB for similar chunks\n",
    "    results = chroma_collection.query(query_texts=[query], n_results=n_results)\n",
    "    \n",
    "    if not results['documents'][0]:\n",
    "        print(\"‚ùå No relevant chunks found!\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìÑ Found {len(results['documents'][0])} relevant chunks:\")\n",
    "    print()\n",
    "    \n",
    "    # Display each retrieved chunk\n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        print(f\"--- Chunk {i+1} ---\")\n",
    "        print(word_wrap(doc))\n",
    "        print()\n",
    "    \n",
    "    return results['documents'][0]\n",
    "\n",
    "# Test retrieval with a sample query\n",
    "print(\"üß™ Testing semantic search retrieval:\")\n",
    "print()\n",
    "chunks = test_retrieval(\"What is TCS's revenue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Step 8: Approach 1 - Basic RAG Implementation\nimport time\n\ndef basic_rag(question):\n    \"\"\"\n    Basic RAG: retrieve 5 chunks, generate answer with GPT-4.1\n    Returns: {\"answer\": str, \"runtime\": float}\n    \"\"\"\n    start_time = time.time()\n\n    # Retrieve 5 most relevant chunks\n    results = chroma_collection.query(query_texts=[question], n_results=5)\n\n    if not results['documents'][0]:\n        runtime = time.time() - start_time\n        return {\n            \"answer\": \"No relevant information found in TCS report.\",\n            \"runtime\": round(runtime, 2)\n        }\n\n    # Combine chunks into context\n    context = \"\\n\\n\".join(results['documents'][0])\n\n    # Generate answer with GPT-4.1\n    try:\n        response = client.responses.create(\n            model=\"gpt-4.1\",\n            input=f\"\"\"Based on the following excerpts from the TCS Annual Report, please answer this question: {question}\n\nContext from TCS Annual Report:\n{context}\n\nPlease provide a clear, accurate answer based only on the information provided above. If the context doesn't contain enough information to fully answer the question, please say so.\"\"\"\n        )\n\n        answer = response.output_text if hasattr(response, 'output_text') else str(response)\n        runtime = time.time() - start_time\n\n        return {\n            \"answer\": answer.strip(),\n            \"runtime\": round(runtime, 2)\n        }\n\n    except Exception as e:\n        runtime = time.time() - start_time\n        return {\n            \"answer\": f\"Error calling OpenAI API: {str(e)}\",\n            \"runtime\": round(runtime, 2)\n        }\n\nprint(\"‚úÖ Basic RAG function implemented!\")\nprint(\"üìã Function signature: basic_rag(question)\")\nprint(\"üì§ Returns: {'answer': str, 'runtime': float}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Step 9: Test Basic RAG Function\n# Test with our standard question to verify it works\n\ntest_question = \"What are TCS's main business segments?\"\nprint(f\"üß™ Testing Basic RAG with: '{test_question}'\")\nprint(\"=\" * 60)\n\nresult = basic_rag(test_question)\n\nprint(f\"üìä Result:\")\nprint(f\"‚è±Ô∏è  Runtime: {result['runtime']}s\")\nprint(f\"üí° Answer:\")\nprint(\"-\" * 40)\nprint(word_wrap(result['answer']))\nprint()\nprint(\"‚úÖ Basic RAG test complete!\")"
  },
  {
   "cell_type": "code",
   "source": "# Approaches 2 & 3: Coming Next\n# Once Basic RAG is tested and working, we'll implement:\n# - Approach 2: Query Expansion (HyDE) \n# - Approach 3: Multiple Queries + Cross-Encoder\n\n# Step 10: Approach 2 - Query Expansion (HyDE) Implementation\n\ndef query_expansion_rag(question):\n    \"\"\"\n    Query Expansion RAG: generate hypothetical answer, combine with question, retrieve 5 chunks\n    Returns: {\"answer\": str, \"runtime\": float, \"hypothetical_answer\": str}\n    \"\"\"\n    start_time = time.time()\n\n    # Step 1: Generate hypothetical answer\n    try:\n        hyp_response = client.responses.create(\n            model=\"gpt-4.1\",\n            input=f\"\"\"You are a helpful expert financial research assistant. Provide an example answer to the given question, that might be found in a document like an annual report.\n\nQuestion: {question}\n\nGenerate a realistic, detailed answer that would typically appear in an annual report:\"\"\"\n        )\n        \n        hypothetical_answer = hyp_response.output_text if hasattr(hyp_response, 'output_text') else str(hyp_response)\n        hypothetical_answer = hypothetical_answer.strip()\n        \n    except Exception as e:\n        # Fallback to original question if hypothetical generation fails\n        hypothetical_answer = \"\"\n    \n    # Step 2: Create expanded query (original + hypothetical)\n    if hypothetical_answer:\n        expanded_query = f\"{question} {hypothetical_answer}\"\n    else:\n        expanded_query = question\n    \n    # Step 3: Retrieve 5 chunks using expanded query\n    results = chroma_collection.query(query_texts=[expanded_query], n_results=5)\n\n    if not results['documents'][0]:\n        runtime = time.time() - start_time\n        return {\n            \"answer\": \"No relevant information found in TCS report.\",\n            \"runtime\": round(runtime, 2),\n            \"hypothetical_answer\": hypothetical_answer\n        }\n\n    # Step 4: Generate final answer using ONLY retrieved context (not hypothetical)\n    context = \"\\n\\n\".join(results['documents'][0])\n\n    try:\n        response = client.responses.create(\n            model=\"gpt-4.1\",\n            input=f\"\"\"Based on the following excerpts from the TCS Annual Report, please answer this question: {question}\n\nContext from TCS Annual Report:\n{context}\n\nPlease provide a clear, accurate answer based only on the information provided above. If the context doesn't contain enough information to fully answer the question, please say so.\"\"\"\n        )\n\n        answer = response.output_text if hasattr(response, 'output_text') else str(response)\n        runtime = time.time() - start_time\n\n        return {\n            \"answer\": answer.strip(),\n            \"runtime\": round(runtime, 2),\n            \"hypothetical_answer\": hypothetical_answer\n        }\n\n    except Exception as e:\n        runtime = time.time() - start_time\n        return {\n            \"answer\": f\"Error calling OpenAI API: {str(e)}\",\n            \"runtime\": round(runtime, 2),\n            \"hypothetical_answer\": hypothetical_answer\n        }\n\nprint(\"‚úÖ Query Expansion RAG function implemented!\")\nprint(\"üìã Function signature: query_expansion_rag(question)\")\nprint(\"üì§ Returns: {'answer': str, 'runtime': float, 'hypothetical_answer': str}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 11: Test Query Expansion RAG Function\n# Test with the same question to compare with Basic RAG\n\ntest_question = \"What are TCS's main business segments?\"\nprint(f\"üß™ Testing Query Expansion RAG with: '{test_question}'\")\nprint(\"=\" * 60)\n\nresult = query_expansion_rag(test_question)\n\nprint(f\"üìä Result:\")\nprint(f\"‚è±Ô∏è  Runtime: {result['runtime']}s\")\nprint(f\"ü§ñ Hypothetical Answer:\")\nprint(\"-\" * 40)\nprint(word_wrap(result['hypothetical_answer']))\nprint()\nprint(f\"üí° Final Answer:\")\nprint(\"-\" * 40)\nprint(word_wrap(result['answer']))\nprint()\nprint(\"‚úÖ Query Expansion RAG test complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 12: Approach 3 - Multiple Queries + Cross-Encoder Implementation\n\n# Import cross-encoder for re-ranking\nfrom sentence_transformers import CrossEncoder\nimport numpy as np\n\n# Initialize cross-encoder (same model as L4-student.md)\ncross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\ndef multiple_queries_rag(question):\n    \"\"\"\n    Multiple Queries RAG: generate 5 related questions, retrieve chunks, cross-encoder re-rank, take top 5\n    Returns: {\"answer\": str, \"runtime\": float, \"generated_queries\": list}\n    \"\"\"\n    start_time = time.time()\n\n    # Step 1: Generate 5 related questions\n    try:\n        queries_response = client.responses.create(\n            model=\"gpt-4.1\",\n            input=f\"\"\"You are a helpful expert financial research assistant. Your users are asking questions about the TCS Annual Report.\n\nSuggest up to 5 additional related questions to help them find the information they need, for the provided question.\nSuggest only short questions without compound sentences. Suggest a variety of questions that cover different aspects of the topic.\nMake sure they are complete questions, and that they are related to the original question.\nOutput one question per line. Do not number the questions.\n\nOriginal question: {question}\n\nGenerate 5 related questions:\"\"\"\n        )\n        \n        content = queries_response.output_text if hasattr(queries_response, 'output_text') else str(queries_response)\n        generated_queries = [q.strip() for q in content.split(\"\\n\") if q.strip()]\n        \n        # Ensure we have exactly 5 questions\n        if len(generated_queries) > 5:\n            generated_queries = generated_queries[:5]\n        elif len(generated_queries) < 5:\n            while len(generated_queries) < 5:\n                generated_queries.append(generated_queries[0] if generated_queries else question)\n                \n    except Exception as e:\n        generated_queries = []\n\n    # Step 2: Create list of all queries (original + 5 related)\n    all_queries = [question] + generated_queries\n\n    # Step 3: Retrieve chunks from all queries and deduplicate\n    all_chunks = []\n    for query in all_queries:\n        try:\n            results = chroma_collection.query(query_texts=[query], n_results=10)  # Get more for better cross-encoder selection\n            if results['documents'][0]:\n                all_chunks.extend(results['documents'][0])\n        except Exception as e:\n            continue\n\n    # Step 4: Deduplicate chunks using exact string matching\n    unique_chunks = []\n    seen_chunks = set()\n    for chunk in all_chunks:\n        if chunk not in seen_chunks:\n            unique_chunks.append(chunk)\n            seen_chunks.add(chunk)\n\n    if not unique_chunks:\n        runtime = time.time() - start_time\n        return {\n            \"answer\": \"No relevant information found in TCS report.\",\n            \"runtime\": round(runtime, 2),\n            \"generated_queries\": generated_queries\n        }\n\n    # Step 5: Cross-encoder re-ranking - score all chunks against original question\n    pairs = [[question, chunk] for chunk in unique_chunks]\n    scores = cross_encoder.predict(pairs)\n\n    # Step 6: Get top 5 highest scoring chunks\n    top_indices = np.argsort(scores)[::-1][:5]  # Top 5 indices\n    top_chunks = [unique_chunks[i] for i in top_indices]\n\n    # Step 7: Generate final answer using top 5 re-ranked chunks\n    context = \"\\n\\n\".join(top_chunks)\n\n    try:\n        response = client.responses.create(\n            model=\"gpt-4.1\",\n            input=f\"\"\"Based on the following excerpts from the TCS Annual Report, please answer this question: {question}\n\nContext from TCS Annual Report:\n{context}\n\nPlease provide a clear, accurate answer based only on the information provided above. If the context doesn't contain enough information to fully answer the question, please say so.\"\"\"\n        )\n\n        answer = response.output_text if hasattr(response, 'output_text') else str(response)\n        runtime = time.time() - start_time\n\n        return {\n            \"answer\": answer.strip(),\n            \"runtime\": round(runtime, 2),\n            \"generated_queries\": generated_queries\n        }\n\n    except Exception as e:\n        runtime = time.time() - start_time\n        return {\n            \"answer\": f\"Error calling OpenAI API: {str(e)}\",\n            \"runtime\": round(runtime, 2),\n            \"generated_queries\": generated_queries\n        }\n\nprint(\"‚úÖ Multiple Queries + Cross-Encoder RAG function implemented!\")\nprint(\"üìã Function signature: multiple_queries_rag(question)\")\nprint(\"üì§ Returns: {'answer': str, 'runtime': float, 'generated_queries': list}\")\nprint(\"üéØ Uses cross-encoder re-ranking for better chunk selection!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 13: Test Multiple Queries + Cross-Encoder RAG Function\n# Test with the same question to compare all three approaches\n\ntest_question = \"What are TCS's main business segments?\"\nprint(f\"üß™ Testing Multiple Queries + Cross-Encoder RAG with: '{test_question}'\")\nprint(\"=\" * 60)\n\nresult = multiple_queries_rag(test_question)\n\nprint(f\"üìä Result:\")\nprint(f\"‚è±Ô∏è  Runtime: {result['runtime']}s\")\nprint(f\"üîç Generated Related Queries:\")\nprint(\"-\" * 40)\nfor i, query in enumerate(result['generated_queries'], 1):\n    print(f\"  {i}. {query}\")\nprint()\nprint(f\"üí° Final Answer (Cross-Encoder Re-ranked):\")\nprint(\"-\" * 40)\nprint(word_wrap(result['answer']))\nprint()\nprint(\"‚úÖ Multiple Queries + Cross-Encoder RAG test complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 18: Quick Test - Compare All Three Approaches\n# Test the three RAG approaches side by side on a sample question\n\ndef compare_all_three_approaches(question):\n    \"\"\"\n    Run all three approaches on a single question for comparison.\n    \"\"\"\n    print(f\"üîç TEST QUESTION: {question}\")\n    print(\"=\" * 80)\n    print()\n    \n    print(\"üîµ APPROACH 1: ORIGINAL RAG\")\n    print(\"-\" * 40)\n    try:\n        original_answer = ask_tcs_report(question)\n        print(\"‚úÖ Original RAG completed\")\n    except Exception as e:\n        original_answer = f\"Error: {str(e)}\"\n        print(f\"‚ùå Original RAG failed: {str(e)}\")\n    \n    print(\"\\\\n\\\\nüü† APPROACH 2: QUERY EXPANSION (HYPOTHETICAL ANSWER)\")  \n    print(\"-\" * 40)\n    try:\n        expansion_answer = ask_tcs_report_with_expansion(question, show_process=False)\n        print(\"‚úÖ Query Expansion (HyDE) completed\")\n    except Exception as e:\n        expansion_answer = f\"Error: {str(e)}\"\n        print(f\"‚ùå Query Expansion failed: {str(e)}\")\n    \n    print(\"\\\\n\\\\nüü¢ APPROACH 3: MULTIPLE QUERY EXPANSION\")\n    print(\"-\" * 40)\n    try:\n        multiple_answer = ask_tcs_report_with_multiple_queries(question, show_process=False)\n        print(\"‚úÖ Multiple Query Expansion completed\")\n    except Exception as e:\n        multiple_answer = f\"Error: {str(e)}\"\n        print(f\"‚ùå Multiple Query Expansion failed: {str(e)}\")\n    \n    print(\"\\\\n\" + \"üîπ\" * 80)\n    print(\"‚úÖ All three approaches tested!\")\n    print(\"üîπ\" * 80)\n    \n    return original_answer, expansion_answer, multiple_answer\n\n# Test with a sample question\nprint(\"üß™ TESTING ALL THREE RAG APPROACHES\")\nprint(\"=\" * 80)\nprint(\"This will show how each approach handles the same question\")\nprint()\n\ntest_question = \"What are TCS's main business segments?\"\nresults = compare_all_three_approaches(test_question)\n\nprint(\"\\\\n\" + \"‚úÖ\" * 50)\nprint(\"üéâ THREE-WAY COMPARISON COMPLETE!\")\nprint(\"‚úÖ\" * 50)\nprint()\nprint(\"üìã What we've tested:\")\nprint(\"  üîµ Original RAG: Standard semantic search with 3 chunks\")\nprint(\"  üü† Query Expansion (HyDE): Hypothetical answer + original query\") \nprint(\"  üü¢ Multiple Query Expansion: Original + 5 related questions\")\nprint()\nprint(\"üéØ Next: Generate answers for all 15 evaluation questions using the new approach!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 14: Summary - Three RAG Approaches Implemented\n\nprint(\"üéâ TCS RAG SYSTEM COMPLETE!\")\nprint(\"=\" * 50)\nprint()\nprint(\"üìã Three approaches implemented and ready for testing:\")\nprint()\nprint(\"üîµ 1. Basic RAG\")\nprint(\"   ‚Ä¢ Function: basic_rag(question)\")\nprint(\"   ‚Ä¢ Retrieves 5 chunks directly\")\nprint(\"   ‚Ä¢ Returns: {'answer': str, 'runtime': float}\")\nprint()\nprint(\"üü† 2. Query Expansion (HyDE)\")  \nprint(\"   ‚Ä¢ Function: query_expansion_rag(question)\")\nprint(\"   ‚Ä¢ Generates hypothetical answer for better retrieval\")\nprint(\"   ‚Ä¢ Returns: {'answer': str, 'runtime': float, 'hypothetical_answer': str}\")\nprint()\nprint(\"üü¢ 3. Multiple Queries + Cross-Encoder\")\nprint(\"   ‚Ä¢ Function: multiple_queries_rag(question)\")\nprint(\"   ‚Ä¢ Generates 5 related questions, cross-encoder re-ranks\")\nprint(\"   ‚Ä¢ Returns: {'answer': str, 'runtime': float, 'generated_queries': list}\")\nprint()\nprint(\"üß™ All functions tested with: 'What are TCS's main business segments?'\")\nprint(\"‚úÖ Ready for evaluation and comparison!\")\nprint()\nprint(\"üéØ Next steps:\")\nprint(\"   ‚Ä¢ Test with different questions\")\nprint(\"   ‚Ä¢ Compare answer quality across approaches\") \nprint(\"   ‚Ä¢ Build evaluation framework when needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}